package main

import (
	"context"
	"fmt"
	"log"

	"github.com/apache/spark-connect-go/v35/spark/sql"
	"github.com/apache/spark-connect-go/v35/spark/sql/functions"
	"github.com/grundprinzip/unofficial-dbconnect-go/dbconnect"

	"github.com/databricks/{{.project_name}}/transforms"
)

func main() {
	fmt.Println("Running my workload from Databricks")
	ctx := context.Background()
	cb := dbconnect.NewDataBricksChannelBuilder()

	// We can use the config to leverage the unified auth provided
	// by all Databricks SDKs.
	// cfg := config.Config{Profile: "DEFAULT"}
	// cb = cb.WithConfig(&cfg)

	// We can actually configure the session using different things.
	// Configure to use serverless...
	// cb = cb.UseServerless()

	// We can set a cluster ID
	// cb.UseCluster("2024-05-06-123456-123456")

	// But by default we just use magic...
	spark, err := sql.NewSessionBuilder().WithChannelBuilder(cb).Build(ctx)
	if err != nil {
		fmt.Printf("Failed: %s", err)
	}

	df, err := spark.Table("samples.nyctaxi.trips")
	if err != nil {
		log.Fatal(err)
	}

  // Load one of the transformations from the package directory.
	df, err = transforms.Transform(ctx, df)
	if err != nil {
	  log.Fatal(err)
	}

	df, err = df.GroupBy(
		functions.Round(functions.Col("fare_amount"), 0)).Sum(ctx, "fare_amount")
	if err != nil {
		log.Fatal(err)
	}

	err = df.Show(ctx, 100, false)
	if err != nil {
		log.Fatalf("Failed: %s", err)
	}
	fmt.Printf("Done...")
}
